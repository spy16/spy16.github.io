<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Scheduled Job Queue - spy16</title><meta name=viewport content="width=device-width,initial-scale=1"><meta itemprop=name content="Scheduled Job Queue"><meta itemprop=description content="Building a reliable & efficient, scheduled job-queue with Redis."><meta itemprop=datePublished content="2022-11-19T11:01:59+05:30"><meta itemprop=dateModified content="2022-11-19T11:01:59+05:30"><meta itemprop=wordCount content="1622"><meta itemprop=keywords content="distributed_system,redis,scheduler,cron,jobqueue,"><meta property="og:title" content="Scheduled Job Queue"><meta property="og:description" content="Building a reliable & efficient, scheduled job-queue with Redis."><meta property="og:type" content="article"><meta property="og:url" content="https://spy16.in/posts/scheduled-job-queue/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-19T11:01:59+05:30"><meta property="article:modified_time" content="2022-11-19T11:01:59+05:30"><meta name=twitter:card content="summary"><meta name=twitter:title content="Scheduled Job Queue"><meta name=twitter:description content="Building a reliable & efficient, scheduled job-queue with Redis."><link href="https://fonts.googleapis.com/css?family=Playfair+Display:700" rel=stylesheet type=text/css><link rel=stylesheet type=text/css media=screen href=https://spy16.in/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://spy16.in/css/main.css><link id=dark-scheme rel=stylesheet type=text/css href=https://spy16.in/css/dark.css><script src=https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js></script><script src=https://spy16.in/js/main.js></script><script defer type=text/javascript src=https://api.pirsch.io/pirsch.js id=pirschjs data-code=rOdoEa5tg2DPkb3UctA9foeQmue2YpPH></script></head><body><div class="container wrapper"><div class=header><div class=avatar><a href=https://spy16.in/><img src="https://s.gravatar.com/avatar/07d369134a5308bfc99d1d5ae985e7f1?s=80" alt=spy16></a></div><h1 class=site-title><a href=https://spy16.in/>spy16</a></h1><div class=site-description><p>Poor attempts to convey my thoughts.</p><nav class="nav social"><ul class=flat><li><a href=https://github.com/spy16 title=Github><i data-feather=github></i></a></li><li><a href=https://www.linkedin.com/in/shivaprasadbhat/ title=Github><i data-feather=linkedin></i></a></li><li><a href=/index.xml title=RSS><i data-feather=rss></i></a></li><li><a href=# class=scheme-toggle id=scheme-toggle></a></li></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Home</a></li><li><a href=/posts>All Posts</a></li><li><a href=/tags>Tags</a></li><li><a href=/about>About Me</a></li></ul></nav></div><div class=post><div class=post-head><div class=post-header><div class=meta><div class=date><span class=day>19</span>
<span class=rest>Nov 2022</span></div></div><div class=matter><h1 class=title>Scheduled Job Queue</h1></div></div><div class=tags><ul class=flat><li><a href=/tags/distributed_system>distributed_system</a></li><li><a href=/tags/redis>redis</a></li><li><a href=/tags/scheduler>scheduler</a></li><li><a href=/tags/cron>cron</a></li><li><a href=/tags/jobqueue>jobqueue</a></li></ul></div></div><button class=collapsible> ℹ️ &nbsp; Table of Contents</button><aside class="collapsible-content toc"><nav id=TableOfContents><ul><li><a href=#problem>Problem</a></li><li><a href=#using-a-priority-queue>Using a Priority Queue</a></li><li><a href=#need-for-state-storage>Need for state-storage</a></li><li><a href=#using-redis>Using Redis</a><ul><li><a href=#fifo-queues-with-lists>FIFO Queues with Lists</a></li><li><a href=#scheduled-queues-with-sorted-sets>Scheduled Queues with Sorted-Sets</a></li><li><a href=#sharding>Sharding</a></li></ul></li></ul></nav></aside><div class=markdown><p>I was working on a product where we needed the ability to schedule certain background jobs to happen at specific time in the future. After evaluating certain existing solutions including Google Cloud Scheduler, Dkron, etc., we decided to build something internally due to either reliability, scale, cost or security reasons.</p><h2 id=problem>Problem</h2><p>At a high level, we can think of what we need as a special kind of job-queue. In a typical job/task-queue, a job is ready as soon as it is enqueued. A worker will dequeue and process as soon as possible. But for our requirement, we need a job queue where we can enqueue each job with a ready-time and it can be dequeued only after that ready-time.</p><p>A simple illustration of this requirement using a normal job-queue would be:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>enqueue</span>(job, ready_at):
    sleep_until(ready_at)
    typical_job_queue<span style=color:#666>.</span>enqueue(job)

<span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>dequeue</span>():
    <span style=color:#a2f;font-weight:700>return</span> typical_job_queue<span style=color:#666>.</span>dequeue()
</code></pre></div><p>Obviously, sleeping isn&rsquo;t a feasible strategy at all. What if we have thousands of jobs that need to be executed at specific times?, What if the program crashes while it is in sleep?, etc.</p><p>It would be ideal if the underlying job-queue system itself provides a feature to set &ldquo;ready time&rdquo; while enqueing jobs and allows only &ldquo;ready items&rdquo; to be dequeued.</p><h2 id=using-a-priority-queue>Using a Priority Queue</h2><p>We could use a priority-queue (like min-heap) and store items with their <code>readyTime</code> as the priority. Then, we simply peek the queue to see if the item at the front is ready, prevent dequeue if not.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-golang data-lang=golang><span style=color:#a2f;font-weight:700>func</span> (pq <span style=color:#666>*</span>PriorityQueue) <span style=color:#00a000>Enqueue</span>(readyAt time.Time, data []<span style=color:#0b0;font-weight:700>byte</span>) <span style=color:#0b0;font-weight:700>error</span> {
    pq.<span style=color:#00a000>push</span>(
        readyAt <span style=color:#080;font-style:italic>/* priority of this item */</span>,
        data    <span style=color:#080;font-style:italic>/* the item value */</span>,
    )
    <span style=color:#a2f;font-weight:700>return</span> <span style=color:#a2f;font-weight:700>nil</span>
}

<span style=color:#a2f;font-weight:700>func</span> (pq <span style=color:#666>*</span>PriorityQueue) <span style=color:#00a000>Dequeue</span>(relTime time.Time) ([]<span style=color:#0b0;font-weight:700>byte</span>, <span style=color:#0b0;font-weight:700>error</span>) {
    priority, _ = pq.<span style=color:#00a000>peek</span>()
    <span style=color:#a2f;font-weight:700>if</span> priority &gt; relTime {
        <span style=color:#080;font-style:italic>// No job is ready yet. Try dequeue again after sometime.
</span><span style=color:#080;font-style:italic></span>        <span style=color:#a2f;font-weight:700>return</span> <span style=color:#a2f;font-weight:700>nil</span>, ErrNoReadyItem
    }

    <span style=color:#080;font-style:italic>// The item at the front of the queue is ready.
</span><span style=color:#080;font-style:italic></span>    <span style=color:#a2f;font-weight:700>return</span> pq.<span style=color:#00a000>pop</span>()
}
</code></pre></div><p>This is definitely better than the <code>sleep_until</code> model, but this is still in-memory and does not support distributed setup needed for a highly-available system (Unless we can implement a persistent & distributed priority-queue).</p><h2 id=need-for-state-storage>Need for state-storage</h2><p>When we think about distributed setup for this job-queue, we also run into another major problem: What happens when a worker removes a ready job for processing by doing a <code>Dequeue()</code> but crashes immediately? Dequeue would have removed the item from the queue and now it is lost. We cannot leave the item in the queue until successful processing also because other workers might try to claim the same job and process it again.</p><p>We need some state storage that we can use to co-ordinate all our workers and achieve the following objectives:</p><ol><li>If a worker crashes, the overall system should continue working without causing partial/complete downtime</li><li>A job or set of jobs should NOT be impacted by unavailability of a specific worker.</li><li>A worker crash should NOT cause data loss (i.e., <code>at-least-once</code> guarantee is a must).</li><li>As the number of jobs grow, we should be able to add new workers easily and the system should somehow distribute the work evenly across all available workers.</li></ol><h2 id=using-redis>Using Redis</h2><p>Redis is in-memory, fast, and provides various data structures that we can use. List and sorted-set in particular are very useful for building job queues. For this reason, it is used in many typical job-queue frameworks as well (e.g., gocraft/work) and we will also develop our delayed-queue with Redis in following sections.</p><h3 id=fifo-queues-with-lists>FIFO Queues with Lists</h3><p>A normal FIFO job-queue can be implemented using list data-structure by combining <code>LPUSH</code> &
<code>BRPOP</code> commands <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. But the problem with this design is that if the worker that picks up
the job crashes, the job is lost because it has been removed from the <code>jobs</code> list. To support
recovery, the popped item should be moved to an ongoing list atomically and a recovery process
should reconcile these items somehow if worker crashes. This can be accomplished using the Lua support for example <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. But it still lacks the time-constraitnt that we want to support. Sorted-Sets to the rescue!</p><h3 id=scheduled-queues-with-sorted-sets>Scheduled Queues with Sorted-Sets</h3><p>A Redis sorted set is a collection of unique strings (members) ordered by an associated score<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><h4 id=enqueue>Enqueue</h4><p>Let&rsquo;s say we have have a sorted-set named <code>delay_set</code>. We can store jobs in this set just like we can with a normal List. But in addition, we can also attach a priority/score to each item. Since we need to order items based on their ready-time and it is monotonically increasing, we can simply use the ready-time itself as the priority and the set gets sorted naturally. So our enqueue will look like:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>enqueue</span>(job, ready_at):
    <span style=color:#080;font-style:italic># seconds since unix epoch</span>
    unix_ts <span style=color:#666>=</span> ready_at<span style=color:#666>.</span>unix_seconds()

    <span style=color:#080;font-style:italic># add the item to &#39;delay_set&#39; with &#39;unix_ts&#39; as score.</span>
    <span style=color:#080;font-style:italic># ZADD has O(log(n)) time-complexity.</span>
    redis_call(<span style=color:#b44>&#34;ZADD&#34;</span>, key<span style=color:#666>=</span><span style=color:#b44>&#34;delay_set&#34;</span>, item<span style=color:#666>=</span>job, score<span style=color:#666>=</span>unix_ts)
</code></pre></div><p><code>ZADD</code> has <code>O(log(n))</code> time-complexity<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</p><p>Since Redis commands are atomic, multiple clients can do <code>enqueue()</code> concurrently without any issues, Great! Now, we need a way to <code>dequeue()</code>.</p><h4 id=dequeue>Dequeue</h4><p>Since we are using the ready-time itself as the score of each item in our <code>delay_set</code>, we can assume any item in the set that has score less than current-timestamp as ready-job.</p><p>So, we can find the ready-jobs by doing a <code>ZRANGE</code> with <code>[-inf, now().unix_seconds()]</code> as the scan range. For example, assuming the time right now is <code>1668842026</code>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback>ZRANGE delay_set -inf 1668842026 BYSCORE
</code></pre></div><p>This will return (without removing) all items in the set that have scores less than current time (i.e., items that are ready for processing). But this is where it gets a little tricky. As mentioned earlier, <code>dequeue()</code> SHOULD NOT remove item from the Redis until it is successfully processed to prevent data loss. On the other hand, it cannot leave it in the set <em>as-is</em> also because then other workers will have no way of knowing that job is already being processed. So we need some way to leave the job in Redis, but somehow prevent other workers from &ldquo;seeing&rdquo; it as ready. There are many ways to do this ranging from locks to secondary container for ongoing jobs, etc.</p><p>An interesting technique that I found out is to keep the item in the set, but simply increase the score of that item by some configurable period. So everytime the worker polls the <code>delay_set</code> for jobs, it should</p><ol><li>Execute <code>ZRANGE</code> by-score with <code>[-inf, now()]</code> score range.</li><li>For each item returned, it should increase the score by configurable period (call it <code>reclaim_ttl</code>).</li></ol><p>This way, once a worker picks up a job, it will not appear in polls done by other workers for sometime. If worker finishes processing successfully, it can acknowledge by removing the item permanently using <code>ZREM</code>. If the worker that picked up the job crashes, it&rsquo;s not a problem since other workers will start seeing this job as ready after <code>reclaim_ttl</code> is over. One important point to note is that, this technique also implies that the worker that picked up the job has at-most <code>reclaim_ttl</code> time to finish and acknowledge it. If not done, it can lead to duplicate processing of the same item. But it&rsquo;s easy to prevent by tuning the <code>reclaim_ttl</code> and also by using this time to move the job request to another normal job-queue and acknowledge instead of actually executing business logic. This technique single-handedly resolves most issues we have!</p><p>Unfortunately, Redis does not provide a built-in command do these steps atomically. Fortunately though, it does provide Lua support with atomicity guarantees which we can use to build custom atomic commands by combining built-in commands. Nice!</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-lua data-lang=lua><span style=color:#a2f;font-weight:700>local</span> from_set <span style=color:#666>=</span> KEYS[<span style=color:#666>1</span>]
<span style=color:#a2f;font-weight:700>local</span> max_score, new_score, batch_sz <span style=color:#666>=</span> ARGV[<span style=color:#666>1</span>], ARGV[<span style=color:#666>2</span>] <span style=color:#a2f;font-weight:700>or</span> <span style=color:#666>0.0</span>, ARGV[<span style=color:#666>3</span>]

<span style=color:#a2f;font-weight:700>local</span> items <span style=color:#666>=</span> redis.call(<span style=color:#b44>&#39;ZRANGE&#39;</span>, from_set, <span style=color:#b44>&#39;-inf&#39;</span>, max_score, <span style=color:#b44>&#39;BYSCORE&#39;</span>,
                            <span style=color:#b44>&#39;LIMIT&#39;</span>, <span style=color:#666>0</span>, batch_sz)
<span style=color:#a2f;font-weight:700>for</span> i, value <span style=color:#a2f;font-weight:700>in</span> ipairs(items) <span style=color:#a2f;font-weight:700>do</span>
    redis.call(<span style=color:#b44>&#39;ZADD&#39;</span>, from_set, <span style=color:#b44>&#39;XX&#39;</span>, new_score, value)
<span style=color:#a2f;font-weight:700>end</span>
<span style=color:#a2f;font-weight:700>return</span> items
</code></pre></div><p>The <code>ZRANGE</code> has <code>O(log(N) + M)</code> complexity <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> where M is number of items returned. Which can vary from 0 to batch_sz. The loop is <code>O(M)</code>, <code>ZADD</code> is <code>O(log N)</code> <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> and together is <code>O(M*log(N))</code> complexity. So overall, the Lua script itself has O(M*log(N)) complexity (dropping the non-significant part).</p><p>Dequeue implementation would simply be to invoke this Lua script now.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>dequeue_batch</span>(batch_size<span style=color:#666>=</span><span style=color:#666>100</span>):
    max_score <span style=color:#666>=</span> now()<span style=color:#666>.</span>unix_seconds()
    new_score <span style=color:#666>=</span> max_score <span style=color:#666>+</span> <span style=color:#666>30</span> <span style=color:#080;font-style:italic># reclaim_ttl = 30 seconds</span>
    ready_jobs <span style=color:#666>=</span> exec_lua(
        keys<span style=color:#666>=</span>[<span style=color:#b44>&#34;delay_set&#34;</span>],
        args<span style=color:#666>=</span>[max_score, new_score, batch_size],
    )

    <span style=color:#080;font-style:italic># This loop MUST be finished in 30 seconds. Otherwise, it can</span>
    <span style=color:#080;font-style:italic># lead to duplicate processing.</span>
    <span style=color:#a2f;font-weight:700>for</span> job <span style=color:#a2f;font-weight:700>in</span> ready_jobs:
        <span style=color:#080;font-style:italic># enqueue the job into another normal job-queue or publish</span>
        <span style=color:#080;font-style:italic># to a message-queue (e.g., Kafka), etc.</span>
        execute_job(job)

        <span style=color:#080;font-style:italic># acknowledge by removing the item.</span>
        redis_call(<span style=color:#b44>&#34;ZREM&#34;</span>, <span style=color:#b44>&#34;delay_set&#34;</span>, job)
</code></pre></div><p>The <code>ZREM</code> command has <code>O(M*log(N))</code> <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>. Effectively, dequeue has <code>O(M*log(N))</code> complexity.</p><p>Workers can invoke the <code>dequeue_batch()</code> function repeatedly to keep processing items as they become ready. If a worker from the pool crashes, it makes no difference to overall health of the system because other workers will still continue processing any jobs that are ready. Adding new worker to the pool is also seamless because, the worker starts polling as soon as it starts up without disturbing anything else.</p><h3 id=sharding>Sharding</h3><p>One thing to note is that we are using a single set to store all jobs which might grow signficanlty over time. This can have impact on the RAM & disk requirements of the Redis node. Also, since Redis is single-threaded, at a time, only one worker is actually able to poll the set. If Redis cluster is an option, we can resolve this by sharding the <code>delay_set</code>. For example, we can shard the delay_set into 12 shards (e.g., delay_set_0, delay_set_1, etc). Redis will distribute these across nodes/slots in the cluster. During enqueue/dequeue, we can randomly pick a shard and use. This way, chances of multiple workers trying to poll the same key on the same node reduces.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p><a href=https://redis.com/ebook/part-2-core-concepts/chapter-6-application-components-in-redis/6-4-task-queues/6-4-1-first-in-first-out-queues/>https://redis.com/ebook/part-2-core-concepts/chapter-6-application-components-in-redis/6-4-task-queues/6-4-1-first-in-first-out-queues/</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p><a href=https://redis.io/commands/lmove#pattern-reliable-queue>https://redis.io/commands/lmove#pattern-reliable-queue</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p><a href=https://redis.io/docs/data-types/sorted-sets/>https://redis.io/docs/data-types/sorted-sets/</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p><a href=https://redis.io/commands/zadd/>https://redis.io/commands/zadd/</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p><a href=https://redis.io/commands/zrange/>https://redis.io/commands/zrange/</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6 role=doc-endnote><p><a href=https://redis.io/commands/zrem/>https://redis.io/commands/zrem/</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><hr><h3>See Also</h3><ul><li><a href=/posts/building-a-scheduler/>Building a distributed cron</a></li></ul><hr id=disqus_separator><div id=disqus_thread></div><script type=text/javascript>(function(){var a,b;if(window.location.hostname=="localhost"){document.getElementById('disqus_separator').hidden=!0;return}a=document.createElement('script'),a.type='text/javascript',a.async=!0,b='spy16-in',a.src='//'+b+'.disqus.com/embed.js',(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(a)})()</script><noscript>Please enable JavaScript to view the</a></noscript></div><div class="footer wrapper"><nav class=nav><div>2023 © Shivaprasad Bhat
| Customised
<a href=https://github.com/knadh/hugo-ink>Ink</a> theme on
<a href=https://gohugo.io>Hugo</a></div></nav><script src=https://app.fastbots.ai/embed.js data-bot-id=clgq842q10001lc0z7mnng43n async></script></div><script async src="https://www.googletagmanager.com/gtag/js?id=G-E57FGYYMSF"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-E57FGYYMSF',{anonymize_ip:!1})}</script><script>feather.replace()</script></div></body><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0})</script><script>var coll=document.getElementsByClassName("collapsible"),i;for(i=0;i<coll.length;i++)coll[i].addEventListener("click",function(){this.classList.toggle("active");var a=this.nextElementSibling;a.style.display==="block"?a.style.display="none":a.style.display="block"})</script></html>