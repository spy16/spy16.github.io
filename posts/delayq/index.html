<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>DelayQ - spy16</title><meta name=viewport content="width=device-width,initial-scale=1"><meta itemprop=name content="DelayQ"><meta itemprop=description content="Building a reliable, efficient delayed task-queue or job-queue with Redis."><meta itemprop=datePublished content="2022-03-03T11:01:59+05:30"><meta itemprop=dateModified content="2022-03-03T11:01:59+05:30"><meta itemprop=wordCount content="1613"><meta itemprop=keywords content="redis,scheduler,crontab,job-queue,"><meta property="og:title" content="DelayQ"><meta property="og:description" content="Building a reliable, efficient delayed task-queue or job-queue with Redis."><meta property="og:type" content="article"><meta property="og:url" content="https://spy16.in/posts/delayq/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-03-03T11:01:59+05:30"><meta property="article:modified_time" content="2022-03-03T11:01:59+05:30"><meta name=twitter:card content="summary"><meta name=twitter:title content="DelayQ"><meta name=twitter:description content="Building a reliable, efficient delayed task-queue or job-queue with Redis."><link href="https://fonts.googleapis.com/css?family=Playfair+Display:700" rel=stylesheet type=text/css><link rel=stylesheet type=text/css media=screen href=https://spy16.in/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://spy16.in/css/main.css><link id=dark-scheme rel=stylesheet type=text/css href=https://spy16.in/css/dark.css><script src=https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js></script><script src=https://spy16.in/js/main.js></script></head><body><div class="container wrapper"><div class=header><div class=avatar><a href=https://spy16.in/><img src="https://s.gravatar.com/avatar/07d369134a5308bfc99d1d5ae985e7f1?s=80" alt=spy16></a></div><h1 class=site-title><a href=https://spy16.in/>spy16</a></h1><div class=site-description><p>Poor attempts to convey my thoughts.</p><nav class="nav social"><ul class=flat><li><a href=https://github.com/spy16 title=Github><i data-feather=github></i></a></li><li><a href=/index.xml title=RSS><i data-feather=rss></i></a></li><li><a href=# class=scheme-toggle id=scheme-toggle></a></li></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Home</a></li><li><a href=/posts>All Posts</a></li><li><a href=/tags>Tags</a></li><li><a href=/about>About Me</a></li></ul></nav></div><div class=post><div class=post-head><div class=post-header><div class=meta><div class=date><span class=day>03</span>
<span class=rest>Mar 2022</span></div></div><div class=matter><h1 class=title>DelayQ</h1></div></div><div class=tags><ul class=flat><li><a href=/tags/redis>redis</a></li><li><a href=/tags/scheduler>scheduler</a></li><li><a href=/tags/crontab>crontab</a></li><li><a href=/tags/job-queue>job-queue</a></li></ul></div></div><div class=markdown><p>I was working on a product where we needed the ability to schedule certain background jobs to happen at specific time in the future. After evaluating certain existing solutions including Google Cloud Scheduler, Dkron, etc., we decided to build something internally due to either reliability, scale, cost or security reasons.</p><p>At a high level, we can think of this problem as a special kind of job-queue. In a typical job/task-queue, a job is ready as soon as it is enqueued. A worker will dequeue and process as soon as possible. But for our requirement, we need a job queue where we can enqueue each job with a ready-time and it can be dequeued only after that ready-time.</p><p>A simple illustration of this requirement using a normal job-queue would be:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=font-weight:700>def</span> <span style=color:#900;font-weight:700>enqueue</span>(job, ready_at):
    sleep_until(ready_at)
    typical_job_queue<span style=font-weight:700>.</span>enqueue(job)

<span style=font-weight:700>def</span> <span style=color:#900;font-weight:700>dequeue</span>():
    <span style=font-weight:700>return</span> typical_job_queue<span style=font-weight:700>.</span>dequeue()
</code></pre></div><p>Obviously, sleeping isn&rsquo;t a feasible strategy at all. What if we have thousands of jobs that need to be executed at specific times?, What if the program crashes while it is in sleep?, etc.</p><p>It would be ideal if the underlying job-queue system itself provides a feature to set &ldquo;ready time&rdquo; while enqueing jobs and allows only &ldquo;ready items&rdquo; to be dequeued.</p><p>In other words (or code), this is the interface we would like:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-golang data-lang=golang><span style=font-weight:700>type</span> DelayQueue <span style=font-weight:700>interface</span> {
    <span style=color:#998;font-style:italic>// Enqueue should ensure that the `job` becomes ready
</span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// for dequeue only at `readyTime`.
</span><span style=color:#998;font-style:italic></span>    <span style=color:#900;font-weight:700>Enqueue</span>(readyTime time.Time, job []<span style=color:#458;font-weight:700>byte</span>) <span style=color:#458;font-weight:700>error</span>

    <span style=color:#998;font-style:italic>// Dequeue should return a &#34;ready item&#34; (i.e., An item
</span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// that was enqueued with `redyTime &lt;= relativeTo`).
</span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// For example, passing `time.Now()` should return an
</span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// item that is ready right now.
</span><span style=color:#998;font-style:italic></span>    <span style=color:#900;font-weight:700>Dequeue</span>(relativeTo time.Time) (job []<span style=color:#458;font-weight:700>byte</span>, err <span style=color:#458;font-weight:700>error</span>)
}
</code></pre></div><p>We could use a priority-queue (like min-heap) and store items with their <code>readyTime</code> as the priority. Then, we simply peek the queue to see if the item at the front is ready, prevent dequeue if not.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-golang data-lang=golang><span style=font-weight:700>func</span> (pq <span style=font-weight:700>*</span>PriorityQueue) <span style=color:#900;font-weight:700>Enqueue</span>(readyAt time.Time, data []<span style=color:#458;font-weight:700>byte</span>) <span style=color:#458;font-weight:700>error</span> {
    pq.<span style=color:#900;font-weight:700>push</span>(
        readyAt <span style=color:#998;font-style:italic>/* priority of this item */</span>,
        data    <span style=color:#998;font-style:italic>/* the item value */</span>,
    )
    <span style=font-weight:700>return</span> <span style=font-weight:700>nil</span>
}

<span style=font-weight:700>func</span> (pq <span style=font-weight:700>*</span>PriorityQueue) <span style=color:#900;font-weight:700>Dequeue</span>(relTime time.Time) ([]<span style=color:#458;font-weight:700>byte</span>, <span style=color:#458;font-weight:700>error</span>) {
    priority, _ = pq.<span style=color:#900;font-weight:700>peek</span>()
    <span style=font-weight:700>if</span> priority &gt; relTime {
        <span style=color:#998;font-style:italic>// No job is ready yet. Try dequeue again after sometime.
</span><span style=color:#998;font-style:italic></span>        <span style=font-weight:700>return</span> <span style=font-weight:700>nil</span>, ErrNoReadyItem
    }

    <span style=color:#998;font-style:italic>// The item at the front of the queue is ready.
</span><span style=color:#998;font-style:italic></span>    <span style=font-weight:700>return</span> pq.<span style=color:#900;font-weight:700>pop</span>()
}
</code></pre></div><p>This is definitely better than the <code>sleep_until</code> model, but this is still in-memory and does not support distributed setup needed for a highly-available system (Unless we can implement a persistent & distributed priority-queue).</p><p>When we think about distributed setup for this job-queue, we also run into another major problem: What happens when a worker removes a ready job for processing by doing a <code>Dequeue()</code> but crashes immediately? Dequeue would have removed the item from the queue and now it is lost. We cannot leave the item in the queue until successful processing also because other workers might try to claim the same job and process it again.</p><p>We need some state storage that we can use to co-ordinate all our workers and achieve the following objectives:</p><ol><li>If a worker crashes, the overall system should continue working without causing partial/complete downtime</li><li>A job or set of jobs should NOT be impacted by unavailability of a specific worker.</li><li>A worker crash should NOT cause data loss (i.e., <code>at-least-once</code> guarantee is a must).</li><li>As the number of jobs grow, we should be able to add new workers easily and the system should somehow distribute the work evenly across all available workers.</li></ol><p>Redis is in-memory, fast, and provides various data structures that we can use. List and sorted-set in particular are very useful for building job queues. For this reason, it is used in many typical job-queue frameworks as well (e.g., gocraft/work).</p><p>A normal FIFO job-queue can be implemented using list data-structure by combining <code>LPUSH</code> &
<code>BRPOP</code> commands <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. But the problem with this design is that if the worker that picks up
the job crashes, the job is lost because it has been removed from the <code>jobs</code> list. To support
recovery, the popped item should be moved to an ongoing list atomically and a recovery process
should reconcile these items somehow if worker crashes. This can be accomplished using the Lua support for example <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. While this approach might work as normal FIFO job-queue, it still lacks the time-constraitnt that we have. Sorted-Sets to the rescue!</p><p>Let&rsquo;s say we have have a sorted-set named <code>delay_set</code>. We can store jobs in this set just like we can with a normal List. But in addition, we can also attach a priority/score to each item. Since we need to order items based on their ready-time and it is monotonically increasing, we can simply use the ready-time itself as the priority and the set gets sorted naturally. So our enqueue will look like:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=font-weight:700>def</span> <span style=color:#900;font-weight:700>enqueue</span>(job, ready_at):
    <span style=color:#998;font-style:italic># seconds since unix epoch</span>
    unix_ts <span style=font-weight:700>=</span> ready_at<span style=font-weight:700>.</span>unix_seconds()

    <span style=color:#998;font-style:italic># add the item to &#39;delay_set&#39; with &#39;unix_ts&#39; as score.</span>
    <span style=color:#998;font-style:italic># ZADD has O(log(n)) time-complexity.</span>
    redis_call(<span style=color:#b84>&#34;ZADD&#34;</span>, key<span style=font-weight:700>=</span><span style=color:#b84>&#34;delay_set&#34;</span>, item<span style=font-weight:700>=</span>job, score<span style=font-weight:700>=</span>unix_ts)
</code></pre></div><p><code>ZADD</code> has <code>O(log(n))</code> time-complexity<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><p>Since Redis commands are atomic, multiple clients can do <code>enqueue()</code> concurrently without any issues, Great! Now, we need a way to <code>dequeue()</code>.</p><p>We can find the ready-jobs by doing a <code>ZRANGE</code> with <code>[-inf, now().unix_seconds()]</code> as the score range. For example, assuming the time right now is <code>1668842026</code>:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback>ZRANGE delay_set -inf 1668842026 BYSCORE
</code></pre></div><p>This will return (without removing) all items in the set that have scores less than current time (i.e., items that are ready for processing). But this is where it gets a little tricky. As mentioned earlier, <code>dequeue()</code> SHOULD NOT remove item from the Redis until it is successfully processed to prevent data loss. On the other hand, it cannot leave it in the set <em>as-is</em> also because then other workers will have no way of knowing that job is already being processed. So we need some way to leave the job in Redis, but somehow prevent other workers from &ldquo;seeing&rdquo; it as ready. There are many ways to do this ranging from locks to secondary container for ongoing jobs, etc.</p><p>An interesting technique that I found out is to keep the item in the set, but simply increase the score of that item by some configurable period. So everytime the worker polls the <code>delay_set</code> for jobs, it should</p><ol><li>Execute <code>ZRANGE</code> by-score with <code>[-inf, now()]</code> score range.</li><li>For each item returned, it should increase the score by configurable period (call it <code>reclaim_ttl</code>).</li></ol><p>This way, once a worker picks up a job, it will not appear in polls done by other workers for sometime. If worker finishes processing successfully, it can acknowledge by removing the item permanently using <code>ZREM</code>. If the worker that picked up the job crashes, it&rsquo;s not a problem since other workers will start seeing this job as ready after <code>reclaim_ttl</code> is over. One important point to note is that, this technique also implies that the worker that picked up the job has at-most <code>reclaim_ttl</code> time to finish and acknowledge it. If not done, it can lead to duplicate processing of the same item. But it&rsquo;s easy to prevent by tuning the <code>reclaim_ttl</code> and also by using this time to move the job request to another normal job-queue and acknowledge instead of actually executing business logic. This technique single-handedly resolves most issues we have!</p><p>Unfortunately, Redis does not provide a built-in command do these steps atomically. Fortunately though, it does provide Lua support with atomicity guarantees which we can use to build custom atomic commands by combining built-in commands. Nice!</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-lua data-lang=lua><span style=font-weight:700>local</span> from_set <span style=font-weight:700>=</span> KEYS[<span style=color:#099>1</span>]
<span style=font-weight:700>local</span> max_score, new_score, batch_sz <span style=font-weight:700>=</span> ARGV[<span style=color:#099>1</span>], ARGV[<span style=color:#099>2</span>] <span style=font-weight:700>or</span> <span style=color:#099>0.0</span>, ARGV[<span style=color:#099>3</span>]

<span style=font-weight:700>local</span> items <span style=font-weight:700>=</span> redis.call(<span style=color:#b84>&#39;ZRANGE&#39;</span>, from_set, <span style=color:#b84>&#39;-inf&#39;</span>, max_score, <span style=color:#b84>&#39;BYSCORE&#39;</span>)
<span style=font-weight:700>for</span> i, value <span style=font-weight:700>in</span> ipairs(items) <span style=font-weight:700>do</span>
    redis.call(<span style=color:#b84>&#39;ZADD&#39;</span>, from_set, <span style=color:#b84>&#39;XX&#39;</span>, new_score, value)
<span style=font-weight:700>end</span>
<span style=font-weight:700>return</span> items
</code></pre></div><p>The <code>ZRANGE</code> has <code>O(log(N) + M)</code> complexity <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> where M is number of items returned. Which can vary from 0 to batch_sz. The loop is <code>O(M)</code>, <code>ZADD</code> is <code>O(log N)</code> <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> and together is <code>O(M*log(N))</code> complexity. So overall, the Lua script itself has O(M*log(N)) complexity (dropping the non-significant part).</p><p>Dequeue implementation would simply be to invoke this Lua script now.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=font-weight:700>def</span> <span style=color:#900;font-weight:700>dequeue_all_ready</span>(batch_size<span style=font-weight:700>=</span><span style=color:#099>100</span>):
    max_score <span style=font-weight:700>=</span> now()<span style=font-weight:700>.</span>unix_seconds()
    new_score <span style=font-weight:700>=</span> max_score <span style=font-weight:700>+</span> <span style=color:#099>30</span> <span style=color:#998;font-style:italic># reclaim_ttl = 30 seconds</span>
    ready_jobs <span style=font-weight:700>=</span> exec_lua(
        keys<span style=font-weight:700>=</span>[<span style=color:#b84>&#34;delay_set&#34;</span>],
        args<span style=font-weight:700>=</span>[max_score, new_score, batch_size],
    )

    <span style=color:#998;font-style:italic># This loop MUST be finished in 30 seconds. Otherwise, it can</span>
    <span style=color:#998;font-style:italic># lead to duplicate processing.</span>
    <span style=font-weight:700>for</span> job <span style=font-weight:700>in</span> ready_jobs:
        <span style=color:#998;font-style:italic># enqueue the job into another normal job-queue or publish</span>
        <span style=color:#998;font-style:italic># to a message-queue (e.g., Kafka), etc.</span>
        execute_job(job)

        <span style=color:#998;font-style:italic># acknowledge by removing the item.</span>
        redis_call(<span style=color:#b84>&#34;ZREM&#34;</span>, <span style=color:#b84>&#34;delay_set&#34;</span>, job)
</code></pre></div><p>The <code>ZREM</code> command has <code>O(M*log(N))</code> <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>. Effectively, dequeue has <code>O(M*log(N))</code> complexity.</p><p>Workers can invoke the <code>dequeue_all_ready()</code> function repeatedly to keep processing items as they become ready. If a worker from the pool crashes, it makes no difference to overall health of the system because other workers will still continue processing any jobs that are ready. Adding new worker to the pool is also seamless because, the worker starts polling as soon as it starts up without disturbing anything else.</p><p>One thing to note is that we are using a single set to store all jobs which might grow signficanlty over time. This can have impact on the RAM & disk requirements of the Redis node. Also, since Redis is single-threaded, at a time, only one worker is actually able to poll the set. If Redis cluster is an option, we can resole this by sharding the <code>delay_set</code>. For example, we can shard the delay_set into 12 shards. Redis will distribute these across nodes/slots in the cluster. During enqueue/dequeue, we can randomly pick a shard and use. This way, chances of multiple workers trying to poll the same key on the same node reduces.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p><a href=https://redis.com/ebook/part-2-core-concepts/chapter-6-application-components-in-redis/6-4-task-queues/6-4-1-first-in-first-out-queues/>https://redis.com/ebook/part-2-core-concepts/chapter-6-application-components-in-redis/6-4-task-queues/6-4-1-first-in-first-out-queues/</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p><a href=https://redis.io/commands/lmove#pattern-reliable-queue>https://redis.io/commands/lmove#pattern-reliable-queue</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p><a href=https://redis.io/commands/zadd/>https://redis.io/commands/zadd/</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p><a href=https://redis.io/commands/zrange/>https://redis.io/commands/zrange/</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p><a href=https://redis.io/commands/zrem/>https://redis.io/commands/zrem/</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><hr><h3>See Also</h3><ul><li><a href=/posts/building-a-scheduler/>Building a distributed cron</a></li></ul><hr id=disqus_separator><div id=disqus_thread></div><script type=text/javascript>(function(){var a,b;if(window.location.hostname=="localhost"){document.getElementById('disqus_separator').hidden=!0;return}a=document.createElement('script'),a.type='text/javascript',a.async=!0,b='spy16-in',a.src='//'+b+'.disqus.com/embed.js',(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(a)})()</script><noscript>Please enable JavaScript to view the</a></noscript></div><div class="footer wrapper"><nav class=nav><div>2022 © Shivaprasad Bhat | Customised <a href=https://github.com/knadh/hugo-ink>Ink</a> theme on <a href=https://gohugo.io>Hugo</a></div></nav></div><script async src="https://www.googletagmanager.com/gtag/js?id=G-E57FGYYMSF"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-E57FGYYMSF',{anonymize_ip:!1})}</script><script>feather.replace()</script></div></body><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0})</script><script>var coll=document.getElementsByClassName("collapsible"),i;for(i=0;i<coll.length;i++)coll[i].addEventListener("click",function(){this.classList.toggle("active");var a=this.nextElementSibling;a.style.display==="block"?a.style.display="none":a.style.display="block"})</script></html>